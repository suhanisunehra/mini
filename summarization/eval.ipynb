{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f60288d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7803ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c28cea7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Obtaining dependency information for rouge from https://files.pythonhosted.org/packages/32/7c/650ae86f92460e9e8ef969cc5008b24798dcf56a9a8947d04c78f550b3f5/rouge-1.0.1-py3-none-any.whl.metadata\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: six in c:\\users\\suhan\\anaconda3\\lib\\site-packages (from rouge) (1.16.0)\n",
      "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1de8a689",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrouge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Rouge\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Function to read and tokenize text\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_article\u001b[39m(text):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from rouge import Rouge\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Function to read and tokenize text\n",
    "def read_article(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return [sentence.replace(\"[^a-zA-Z0-9]\", \" \") for sentence in sentences]\n",
    "\n",
    "# Calculate cosine similarity between two sentences\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    "    \n",
    "    sent1 = [w.lower() for w in sent1 if w not in stopwords]\n",
    "    sent2 = [w.lower() for w in sent2 if w not in stopwords]\n",
    "\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "    vector1 = [sent1.count(w) for w in all_words]\n",
    "    vector2 = [sent2.count(w) for w in all_words]\n",
    "    \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "# Build similarity matrix for sentences\n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    matrix = np.zeros((len(sentences), len(sentences)))\n",
    "    \n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                matrix[i][j] = sentence_similarity(sentences[i], sentences[j], stop_words)\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "# Generate summary based on top-ranked sentences\n",
    "def generate_summary(text, top_n):\n",
    "    stop_words = stopwords.words('english')\n",
    "    sentences = read_article(text)\n",
    "    similarity_matrix = build_similarity_matrix(sentences, stop_words)\n",
    "    sentence_graph = nx.from_numpy_array(similarity_matrix)\n",
    "    scores = nx.pagerank(sentence_graph)\n",
    "\n",
    "    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
    "    summary = \" \".join([ranked_sentences[i][1] for i in range(min(top_n, len(ranked_sentences)))])\n",
    "    return summary, len(sentences)\n",
    "\n",
    "# Evaluate summaries using ROUGE\n",
    "def evaluate_summary(generated_summary, reference_summary):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(generated_summary, reference_summary)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2aca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"percins/IN-ABS\")\n",
    "articles = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ef9615",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_1_r = rouge_1_p = rouge_1_f = 0\n",
    "rouge_2_r = rouge_2_p = rouge_2_f = 0\n",
    "rouge_l_r = rouge_l_p = rouge_l_f = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d9e397",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, article in enumerate(articles, start=1):\n",
    "    text = article['text']\n",
    "    ground_truth = article['summary']\n",
    "    summary, _ = generate_summary(text, top_n=7)\n",
    "    scores = evaluate_summary(summary, ground_truth)\n",
    "\n",
    "    rouge_1_r += scores[0]['rouge-1']['r']\n",
    "    rouge_1_p += scores[0]['rouge-1']['p']\n",
    "    rouge_1_f += scores[0]['rouge-1']['f']\n",
    "    \n",
    "    rouge_2_r += scores[0]['rouge-2']['r']\n",
    "    rouge_2_p += scores[0]['rouge-2']['p']\n",
    "    rouge_2_f += scores[0]['rouge-2']['f']\n",
    "    \n",
    "    rouge_l_r += scores[0]['rouge-l']['r']\n",
    "    rouge_l_p += scores[0]['rouge-l']['p']\n",
    "    rouge_l_f += scores[0]['rouge-l']['f']\n",
    "\n",
    "    if idx % 100 == 0:\n",
    "        print(f\"{idx} rows complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad0ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_articles = len(articles)\n",
    "print(num_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68eeead",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average ROUGE-1:\", {'r': rouge_1_r / num_articles, 'p': rouge_1_p / num_articles, 'f': rouge_1_f / num_articles})\n",
    "print(\"Average ROUGE-2:\", {'r': rouge_2_r / num_articles, 'p': rouge_2_p / num_articles, 'f': rouge_2_f / num_articles})\n",
    "print(\"Average ROUGE-L:\", {'r': rouge_l_r / num_articles, 'p': rouge_l_p / num_articles, 'f': rouge_l_f / num_articles})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
